{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from depth_image_neural_features.networks import *\n",
    "from depth_image_neural_features.vae import VQVAE\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_non_repeat_ints(max_int, num_ints) -> np.ndarray:\n",
    "    n_repeats = int(np.floor(num_ints / max_int))\n",
    "    if n_repeats == 0:\n",
    "        ints = np.arange(0, max_int, dtype=int)\n",
    "        np.random.shuffle(ints)\n",
    "        ints = ints[:num_ints]\n",
    "    else:\n",
    "        ints = np.arange(0, max_int, dtype=int)\n",
    "        np.random.shuffle(ints)\n",
    "        ints = ints[: (num_ints - n_repeats * max_int)]\n",
    "        for _ in range(n_repeats):\n",
    "            ints_to_cat = np.arange(0, max_int, dtype=int)\n",
    "            np.random.shuffle(ints_to_cat)\n",
    "            ints = np.concatenate((ints, ints_to_cat))\n",
    "    return ints\n",
    "\n",
    "\n",
    "def stack_image(image, n_stackings):\n",
    "    height, width = image.shape\n",
    "    assert width % 2**n_stackings == 0\n",
    "    for n_stack in range(1, n_stackings + 1):\n",
    "        new_width = int(width / 2**n_stack)\n",
    "        image = np.vstack((image[:, :new_width], image[:, new_width:]))\n",
    "    return image\n",
    "\n",
    "\n",
    "class IndexBasedDataset:\n",
    "    def __init__(self, path_to_index, max_distance):\n",
    "        self.max_distance = max_distance\n",
    "        with open(path_to_index, \"r\") as f:\n",
    "            print(\"Loading index\")\n",
    "            self.index = json.load(f)\n",
    "            print(\"Index loaded\")\n",
    "        self.data = self.index[\"data\"]\n",
    "        self.n_images_per_world = dict()\n",
    "        for key in self.data.keys():\n",
    "            self.n_images_per_world[key] = len(self.data[key][\"poses\"])\n",
    "        self.n_images = sum(\n",
    "            [self.n_images_per_world[key] for key in self.n_images_per_world.keys()]\n",
    "        )\n",
    "\n",
    "    def sum_dict(self, d):\n",
    "        suma = 0\n",
    "        for key in d.keys():\n",
    "            suma += d[key]\n",
    "        return suma\n",
    "\n",
    "    def generate_n_pairs_per_world(self, n_pairs):\n",
    "        pairs_per_world = dict()\n",
    "        for world_name in self.data.keys():\n",
    "            pairs_per_world[world_name] = int(\n",
    "                n_pairs / self.n_images * self.n_images_per_world[world_name]\n",
    "            )\n",
    "        n_iter = 0\n",
    "        keys = list(pairs_per_world.keys())\n",
    "        while self.sum_dict(pairs_per_world) < n_pairs:\n",
    "            n_key = n_iter % len(keys)\n",
    "            key = keys[n_key]\n",
    "            pairs_per_world[key] += 1\n",
    "        return pairs_per_world\n",
    "\n",
    "    def set_length(self, length):\n",
    "        if length <= self.n_pairs:\n",
    "            self._len = length\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The manually set length can't be longer that the number of pairs\"\n",
    "            )\n",
    "\n",
    "    def generate_pairs(self, n_pairs):\n",
    "        self.n_pairs = n_pairs\n",
    "        self._len = n_pairs\n",
    "        n_pairs_per_world = self.generate_n_pairs_per_world(n_pairs)\n",
    "        self.n_pairs = n_pairs\n",
    "        self.pairs_index = [[\"world\", 0, 0, np.zeros((0, 0))] for _ in range(n_pairs)]\n",
    "        n_pair = 0\n",
    "        for world_name in tqdm(self.data.keys(), total=len(list(self.data.keys()))):\n",
    "            n_pairs_for_this_world = n_pairs_per_world[world_name]\n",
    "            points_of_this_world = self.data[world_name][\"poses\"]\n",
    "            idxs1 = random_non_repeat_ints(\n",
    "                len(points_of_this_world), n_pairs_for_this_world\n",
    "            )\n",
    "            for idx1 in tqdm(idxs1, total=len(idxs1), leave=False, desc=world_name):\n",
    "                while True:\n",
    "                    idx2 = np.random.randint(0, len(points_of_this_world))\n",
    "                    p1 = np.array(points_of_this_world[idx1][0:3])\n",
    "                    p2 = np.array(points_of_this_world[idx2][0:3])\n",
    "                    vector = p2 - p1\n",
    "                    d = np.linalg.norm(vector, 2)\n",
    "                    if d < self.max_distance:\n",
    "                        self.pairs_index[n_pair][0] = world_name\n",
    "                        self.pairs_index[n_pair][1] = idx1\n",
    "                        self.pairs_index[n_pair][2] = idx2\n",
    "                        self.pairs_index[n_pair][3] = torch.tensor(\n",
    "                            vector[:2].astype(np.float32)\n",
    "                        )\n",
    "                        n_pair += 1\n",
    "                        break\n",
    "        self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        loading_set = set()\n",
    "        for index_element in tqdm(self.pairs_index, desc=\"gen loading set\"):\n",
    "            wn, id1, id2, v = index_element\n",
    "            loading_set.add((wn, id1))\n",
    "            loading_set.add((wn, id2))\n",
    "        loading_set = list(loading_set)\n",
    "        print(\"Alocating_memory\")\n",
    "        self.images = torch.zeros((len(loading_set), 1, 16, 1024))\n",
    "        self.images_dict = dict()\n",
    "        for n, (wn, idx) in enumerate(loading_set):\n",
    "            self.images_dict[(wn, idx)] = n\n",
    "        for n, (wn, idx) in tqdm(\n",
    "            enumerate(loading_set), desc=\"Loading images\", total=len(loading_set)\n",
    "        ):\n",
    "            self.images[n, 0] = torch.Tensor(\n",
    "                np.load(\n",
    "                    os.path.join(self.data[wn][\"images_folder_path\"], f\"{idx:010d}.npy\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wn, id1, id2, v = self.pairs_index[idx]\n",
    "        img1 = self.images[self.images_dict[(wn, id1)]]\n",
    "        img2 = self.images[self.images_dict[(wn, id2)]]\n",
    "        return (img1, img2), v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index\n",
      "Index loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef49f5ef32b4f7ab72416ad014fcf85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d114f68c0b1458daed200459d4d73e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_earthquake:   0%|          | 0/954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0d3454f0344332bb6d350b3cec4c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_cpr:   0%|          | 0/757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f274220f4a6f49999eb654ac860bd88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "canyonview_field:   0%|          | 0/4350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86d9a16ea3141da950ee089b7b4e27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "small_city:   0%|          | 0/15074 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16dde26347b4f379fecdc30a7efc637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_city:   0%|          | 0/2100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8614a61491864080b71878006802add4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_env_large:   0%|          | 0/1066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e54529a2c4d483391a7e8b6ea407a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "waypoint:   0%|          | 0/784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a473d6269a54d748d8ff64bbd823423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_small:   0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8a848aeb2d4750be98444581ba3a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fetchit_challenge_tests_lowlights:   0%|          | 0/415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269e2233d85c452d8d68f36812e50fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "warehouse:   0%|          | 0/829 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081f3a5d891c493788480c87d6cdb22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_zone:   0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a750ae63fb34c7ba7549c22bb99224f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "barrels:   0%|          | 0/4650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fafecb311845e58b7bc683c280740f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "outdoor:   0%|          | 0/10925 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd2e59c636143debbc0f39fbaded85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neighborhood:   0%|          | 0/3556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61f31d3f5c347fbaaef73ce2a262e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "drone_race_track_2018_actual:   0%|          | 0/2534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332289b1a77b48699eb9ba12a8a39cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_cpr_construction:   0%|          | 0/841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ba45f0c2d447b8b8b451e9d18e8693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modified_playpen:   0%|          | 0/640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d3f964d838442a9bea81582703eefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gen loading set:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alocating_memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca15d92355c4f848db58b2d8ca38916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading images:   0%|          | 0/79000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = IndexBasedDataset(\"/home/lorenzo/.datasets/comprehensive_depth_image_dataset/index.json\", max_distance=1)\n",
    "n_samples=50000\n",
    "dataset.generate_pairs(n_samples)\n",
    "# Setup loging\n",
    "logdir = \"/home/lorenzo/.tensorboard\"\n",
    "os.popen(f\"rm -rf {logdir}/**\")\n",
    "writer = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "logdir = \"/home/lorenzo/.tensorboard\"\n",
    "os.popen(f\"rm -rf {logdir}/**\")\n",
    "writer = SummaryWriter(logdir)\n",
    "#model = LastHope()\n",
    "criterion1 = nn.MSELoss(2, reduction=\"mean\")\n",
    "# Training loop\n",
    "n_epochs = 1024\n",
    "bs = 32\n",
    "dropout = 0.1\n",
    "lr = 0.001\n",
    "save_dir = f\"/home/lorenzo/models/retrain_last_hope_with_dropout\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.train()\n",
    "dataset_initial_length = 500\n",
    "for n_increase_of_dataset in range(1000):\n",
    "    counter = 0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    lr_scheduler = ExponentialLR(optimizer, 0.99)\n",
    "    size_of_dataset = dataset_initial_length + n_increase_of_dataset * 300\n",
    "    dataset.set_length(size_of_dataset)\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.9, 0.1])\n",
    "    train_dataloader = DataLoader(train_dataset, bs, shuffle=True)\n",
    "    for n_epoch in tqdm(range(n_epochs), desc=\"Epoch\", leave=True):\n",
    "        model.to(\"cuda\")\n",
    "        train_avg_loss = 0\n",
    "        n_iters = 0\n",
    "        for sample in train_dataloader:\n",
    "            model.train()\n",
    "            # Get data\n",
    "            (img1, img2), labels = sample\n",
    "            img1 = img1.to(\"cuda\")\n",
    "            img2 = img2.to(\"cuda\")\n",
    "            noise1 = torch.tensor(np.random.normal(size=img1.shape) * 2 / 100 + 1).to(\n",
    "                \"cuda\"\n",
    "            )\n",
    "            noise2 = torch.tensor(np.random.normal(size=img2.shape) * 2 / 100 + 1).to(\n",
    "                \"cuda\"\n",
    "            )\n",
    "            img1 *= noise1\n",
    "            img2 *= noise2\n",
    "            labels = labels.to(\"cuda\")\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            result = model(img1,img2)\n",
    "            # calculate loss\n",
    "            train_loss = criterion1(result, labels)\n",
    "            train_avg_loss += train_loss.item()\n",
    "            n_iters +=1\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalars(\n",
    "                f\"{size_of_dataset}/results\",\n",
    "                {\n",
    "                    \"lx\": labels[0, 0],\n",
    "                    \"rx\": result[0, 0],\n",
    "                    \"ly\": labels[0, 1],\n",
    "                    \"ry\": result[0, 1],\n",
    "                },\n",
    "                counter,\n",
    "            )\n",
    "            ## test data\n",
    "            (img1, img2), labels = test_dataset[np.random.randint(0, len(test_dataset))]\n",
    "            img1 = img1.to(\"cuda\")\n",
    "            img2 = img2.to(\"cuda\")\n",
    "            # Add batch dimension\n",
    "            img1 = torch.unsqueeze(img1, 0)\n",
    "            img2 = torch.unsqueeze(img2, 0)\n",
    "            labels = torch.unsqueeze(labels, 0)\n",
    "            labels = labels.to(\"cuda\")\n",
    "            model = model.eval()\n",
    "            result = model(img1, img2)\n",
    "            test_loss = criterion1(result, labels)\n",
    "            writer.add_scalars(\n",
    "                f\"{size_of_dataset}/loss\",\n",
    "                {\"train_loss\": train_loss,\"test_loss\": test_loss},\n",
    "                counter,\n",
    "            )\n",
    "            counter += 1\n",
    "        torch.save(\n",
    "            model.to(\"cpu\").state_dict(), os.path.join(save_dir, f\"last.torch\")\n",
    "        )\n",
    "        if train_avg_loss/n_iters < 0.01:\n",
    "            break\n",
    "        lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lorenzo/repos/depth-image-neural-features/depth_image_neural_features/scripts/train.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lorenzo/repos/depth-image-neural-features/depth_image_neural_features/scripts/train.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39msave(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lorenzo/repos/depth-image-neural-features/depth_image_neural_features/scripts/train.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m             model\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mstate_dict(), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mhome/lorenzo/models\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlast.torch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lorenzo/repos/depth-image-neural-features/depth_image_neural_features/scripts/train.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(\n",
    "            model.to(\"cpu\").state_dict(), os.path.join(\"home/lorenzo/models\", f\"last.torch\")\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
