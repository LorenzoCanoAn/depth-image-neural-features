{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import neptune\n",
    "\n",
    "#torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_non_repeat_ints(max_int, num_ints) -> np.ndarray:\n",
    "    n_repeats = int(np.floor(num_ints / max_int))\n",
    "    if n_repeats == 0:\n",
    "        ints = np.arange(0, max_int, dtype=int)\n",
    "        np.random.shuffle(ints)\n",
    "        ints = ints[:num_ints]\n",
    "    else:\n",
    "        ints = np.arange(0, max_int, dtype=int)\n",
    "        np.random.shuffle(ints)\n",
    "        ints = ints[: (num_ints - n_repeats * max_int)]\n",
    "        for _ in range(n_repeats):\n",
    "            ints_to_cat = np.arange(0, max_int, dtype=int)\n",
    "            np.random.shuffle(ints_to_cat)\n",
    "            ints = np.concatenate((ints, ints_to_cat))\n",
    "    return ints\n",
    "\n",
    "\n",
    "def stack_image(image, n_stackings):\n",
    "    height, width = image.shape\n",
    "    assert width % 2**n_stackings == 0\n",
    "    for n_stack in range(1, n_stackings + 1):\n",
    "        new_width = int(width / 2**n_stack)\n",
    "        image = np.vstack((image[:, :new_width], image[:, new_width:]))\n",
    "    return image\n",
    "\n",
    "\n",
    "class IndexBasedDataset:\n",
    "    def __init__(self, path_to_index, max_distance):\n",
    "        self.max_distance = max_distance\n",
    "        with open(path_to_index, \"r\") as f:\n",
    "            print(\"Loading index\")\n",
    "            self.index = json.load(f)\n",
    "            print(\"Index loaded\")\n",
    "        self.data = self.index[\"data\"]\n",
    "        self.n_images_per_world = dict()\n",
    "        for key in self.data.keys():\n",
    "            self.n_images_per_world[key] = len(self.data[key][\"poses\"])\n",
    "        self.n_images = sum(\n",
    "            [self.n_images_per_world[key] for key in self.n_images_per_world.keys()]\n",
    "        )\n",
    "\n",
    "    def sum_dict(self, d):\n",
    "        suma = 0\n",
    "        for key in d.keys():\n",
    "            suma += d[key]\n",
    "        return suma\n",
    "\n",
    "    def generate_n_pairs_per_world(self, n_pairs):\n",
    "        pairs_per_world = dict()\n",
    "        for world_name in self.data.keys():\n",
    "            pairs_per_world[world_name] = int(\n",
    "                n_pairs / self.n_images * self.n_images_per_world[world_name]\n",
    "            )\n",
    "        n_iter = 0\n",
    "        keys = list(pairs_per_world.keys())\n",
    "        while self.sum_dict(pairs_per_world) < n_pairs:\n",
    "            n_key = n_iter % len(keys)\n",
    "            key = keys[n_key]\n",
    "            pairs_per_world[key] += 1\n",
    "        return pairs_per_world\n",
    "\n",
    "    def set_length(self, length):\n",
    "        if length <= self.n_pairs:\n",
    "            self._len = length\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The manually set length can't be longer that the number of pairs\"\n",
    "            )\n",
    "\n",
    "    def generate_pairs(self, n_pairs):\n",
    "        self.n_pairs = n_pairs\n",
    "        self._len = n_pairs\n",
    "        n_pairs_per_world = self.generate_n_pairs_per_world(n_pairs)\n",
    "        self.n_pairs = n_pairs\n",
    "        self.pairs_index = [[\"world\", 0, 0, np.zeros((0, 0))] for _ in range(n_pairs)]\n",
    "        n_pair = 0\n",
    "        for world_name in tqdm(self.data.keys(), total=len(list(self.data.keys()))):\n",
    "            n_pairs_for_this_world = n_pairs_per_world[world_name]\n",
    "            points_of_this_world = self.data[world_name][\"poses\"]\n",
    "            idxs1 = random_non_repeat_ints(\n",
    "                len(points_of_this_world), n_pairs_for_this_world\n",
    "            )\n",
    "            for idx1 in tqdm(idxs1, total=len(idxs1), leave=False, desc=world_name):\n",
    "                while True:\n",
    "                    idx2 = np.random.randint(0, len(points_of_this_world))\n",
    "                    p1 = np.array(points_of_this_world[idx1][0:3])\n",
    "                    p2 = np.array(points_of_this_world[idx2][0:3])\n",
    "                    vector = p2 - p1\n",
    "                    d = np.linalg.norm(vector, 2)\n",
    "                    if d < self.max_distance:\n",
    "                        self.pairs_index[n_pair][0] = world_name\n",
    "                        self.pairs_index[n_pair][1] = idx1\n",
    "                        self.pairs_index[n_pair][2] = idx2\n",
    "                        self.pairs_index[n_pair][3] = torch.tensor(\n",
    "                            vector[:2].astype(np.float32)\n",
    "                        )\n",
    "                        n_pair += 1\n",
    "                        break\n",
    "        self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        loading_set = set()\n",
    "        for index_element in tqdm(self.pairs_index, desc=\"gen loading set\"):\n",
    "            wn, id1, id2, v = index_element\n",
    "            loading_set.add((wn, id1))\n",
    "            loading_set.add((wn, id2))\n",
    "        loading_set = list(loading_set)\n",
    "        print(\"Alocating_memory\")\n",
    "        self.images = torch.zeros((len(loading_set), 1, 16, 1024))\n",
    "        self.images_dict = dict()\n",
    "        for n, (wn, idx) in enumerate(loading_set):\n",
    "            self.images_dict[(wn, idx)] = n\n",
    "        for n, (wn, idx) in tqdm(\n",
    "            enumerate(loading_set), desc=\"Loading images\", total=len(loading_set)\n",
    "        ):\n",
    "            self.images[n, 0] = torch.Tensor(\n",
    "                np.load(\n",
    "                    os.path.join(self.data[wn][\"images_folder_path\"], f\"{idx:010d}.npy\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wn, id1, id2, v = self.pairs_index[idx]\n",
    "        img1 = self.images[self.images_dict[(wn, id1)]]\n",
    "        img2 = self.images[self.images_dict[(wn, id2)]]\n",
    "        return (img1, img2), v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index\n",
      "Index loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b32df3b0ff49778c0d0eb86976471e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3f827a37d44352b1e49b2a845325a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_earthquake:   0%|          | 0/9468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1315a135bdab45fa8a556d9eaf3a8278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_cpr:   0%|          | 0/7579 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944bd1c3d4e141a2942f66a858abedc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "canyonview_field:   0%|          | 0/43503 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e029ca2bca462db4744fea9d822609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "small_city:   0%|          | 0/150740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5fa9f0733d44f295e50f20efb848e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_city:   0%|          | 0/21006 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d5bcaa9d514addbaff461ca7d86463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_env_large:   0%|          | 0/10662 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737e7ff946b94d6d8e5af962330f0563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "waypoint:   0%|          | 0/7848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af609b94aab04ce898b1a015b0c18543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_small:   0%|          | 0/1720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b55c8b542941e6beb93ff76bf0179d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fetchit_challenge_tests_lowlights:   0%|          | 0/4159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b09ea99e712465ab7321deb002babca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "warehouse:   0%|          | 0/8301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887820820cd1470485d61cfae9b9c962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_zone:   0%|          | 0/3536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac1dc4a53414e6689073dda18b4e5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "barrels:   0%|          | 0/46502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef1be3426a1421c862215e6fd03c00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "outdoor:   0%|          | 0/109247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca770e98fba2441996059e70168f1925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "neighborhood:   0%|          | 0/35565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d57932c0259407a8bbf1756eab5d9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "drone_race_track_2018_actual:   0%|          | 0/25347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f893f6e73c6b4d4a935d991824fdf00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "office_cpr_construction:   0%|          | 0/8412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83012a636bd846918fc3a98a880b3615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modified_playpen:   0%|          | 0/6405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4069991355844826a9e3c42c5c1c08a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gen loading set:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alocating_memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0fa905dede4a4cad5b409630f99091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading images:   0%|          | 0/791675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = IndexBasedDataset(\"/home/lorenzo/datasets/comprehensive_depth_image_dataset/index.json\", max_distance=3)\n",
    "n_samples=500000\n",
    "dataset.generate_pairs(n_samples)\n",
    "# Setup loging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "from depth_image_neural_features.networks import LastHope\n",
    "run = neptune.init_run(\n",
    "    project=\"lcano/depth-image-odom-features\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYjcxZGU4OC00ZjVkLTRmMDAtYjBlMi0wYzkzNDQwOGJkNWUifQ==\",\n",
    ")\n",
    "model = LastHope()\n",
    "criterion1 = nn.MSELoss(2, reduction=\"mean\")\n",
    "# PARAMETERS\n",
    "n_epochs = 1024\n",
    "bs = 128\n",
    "dropout = 0.1\n",
    "lr = 0.001\n",
    "save_dir = f\"/home/lorenzo/models/retrain_last_hope_with_dropout\"\n",
    "dataset_increment = 1000\n",
    "dataset_initial_length = 1000\n",
    "# CODE\n",
    "params = {\n",
    "    \"lr\": lr,\n",
    "    \"bs\": bs,\n",
    "    \"save_dir\": save_dir,\n",
    "    \"dataset_increment\":dataset_increment,\n",
    "    \"dataset_initial_length\":dataset_initial_length,\n",
    "}\n",
    "run[\"parameters\"] = params\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.train()\n",
    "global_counter = 0\n",
    "for n_increase_of_dataset in range(1000):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    lr_scheduler = ExponentialLR(optimizer, 0.99)\n",
    "    size_of_dataset = dataset_initial_length + n_increase_of_dataset * dataset_increment\n",
    "    dataset.set_length(size_of_dataset)\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.9*size_of_dataset), int(0.1*size_of_dataset)])\n",
    "    train_dataloader = DataLoader(train_dataset, bs, shuffle=True)\n",
    "    for n_epoch in tqdm(range(n_epochs), desc=\"Epoch\", leave=True):\n",
    "        model.to(\"cuda\")\n",
    "        train_avg_loss = 0\n",
    "        n_iters = 0\n",
    "        for sample in train_dataloader:\n",
    "            model.train()\n",
    "            # Get data\n",
    "            (img1, img2), labels = sample\n",
    "            img1 = img1.to(\"cuda\")\n",
    "            img2 = img2.to(\"cuda\")\n",
    "            noise1 = torch.tensor(np.random.normal(size=img1.shape) * 2 / 100 + 1).to(\n",
    "                \"cuda\"\n",
    "            )\n",
    "            noise2 = torch.tensor(np.random.normal(size=img2.shape) * 2 / 100 + 1).to(\n",
    "                \"cuda\"\n",
    "            )\n",
    "            img1 *= noise1\n",
    "            img2 *= noise2\n",
    "            labels = labels.to(\"cuda\")\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            result = model(img1,img2)\n",
    "            # calculate loss\n",
    "            train_loss = criterion1(result, labels)\n",
    "            run[f\"train/loss/{size_of_dataset}\"].append(train_loss.item())\n",
    "            train_avg_loss += train_loss.item()\n",
    "            n_iters +=1\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            ## test data\n",
    "            (img1, img2), labels = test_dataset[np.random.randint(0, len(test_dataset))]\n",
    "            img1 = img1.to(\"cuda\")\n",
    "            img2 = img2.to(\"cuda\")\n",
    "            # Add batch dimension\n",
    "            img1 = torch.unsqueeze(img1, 0)\n",
    "            img2 = torch.unsqueeze(img2, 0)\n",
    "            labels = torch.unsqueeze(labels, 0)\n",
    "            labels = labels.to(\"cuda\")\n",
    "            model = model.eval()\n",
    "            result = model(img1, img2)\n",
    "            test_loss = criterion1(result, labels)\n",
    "            run[f\"test/loss/{size_of_dataset}\"].append(test_loss.item())\n",
    "            global_counter += 1\n",
    "\n",
    "        torch.save(\n",
    "            model.to(\"cpu\").state_dict(), os.path.join(save_dir, f\"last.torch\")\n",
    "        )\n",
    "        if train_avg_loss/n_iters < 0.1:\n",
    "            break\n",
    "        lr_scheduler.step()\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "All 0 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/lcano/depth-image-odom-features/e/DEP-16/metadata\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
